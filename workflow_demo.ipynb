{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I have a very simple Python app for saying hello to people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreet\u001b[39m():\n",
      "    parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n",
      "    parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n",
      "    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\n",
      "\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "    greet()\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is just a utility function for printing this file\n",
    "# and others throughout the course of the notebook, this is not\n",
    "# the code for the app itself!!\n",
    "from pygments import highlight\n",
    "from pygments.formatters import Terminal256Formatter\n",
    "from pygments.lexers import PythonLexer\n",
    "\n",
    "def print_file(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read()\n",
    "    print(highlight(text, PythonLexer(), Terminal256Formatter()), end=\"\")\n",
    "\n",
    "print_file(\"app/__main__.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure this app can be deployed reproducibly, we'll build it into an Apptainer container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap: docker\n",
      "From: python:\u001b[38;5;241m3.10\u001b[39m\u001b[38;5;241m.12\u001b[39m\u001b[38;5;241m-\u001b[39mslim\u001b[38;5;241m-\u001b[39mbullseye\n",
      "Stage: build\n",
      "\u001b[38;5;241m%\u001b[39mfiles\n",
      "\u001b[38;5;241m.\u001b[39m \u001b[38;5;241m/\u001b[39mopt\u001b[38;5;241m/\u001b[39mapp\n",
      "\n",
      "\u001b[38;5;241m%\u001b[39mpost\n",
      "python \u001b[38;5;241m-\u001b[39mm pip install \u001b[38;5;241m-\u001b[39me \u001b[38;5;241m/\u001b[39mopt\u001b[38;5;241m/\u001b[39mapp\n"
     ]
    }
   ],
   "source": [
    "print_file(\"app.def\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:   \u001b[0m User not listed in /etc/subuid, trying root-mapped namespace\n",
      "\u001b[34mINFO:   \u001b[0m The %post section will be run under fakeroot\n",
      "\u001b[34mINFO:   \u001b[0m Starting build...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting image source signatures\n",
      "Copying blob 63cd35141f3a skipped: already exists  \n",
      "\u001b[1A\u001b[JCopying blob 63cd35141f3a skipped: already exists  \n",
      "Copying blob 7d676dc8a994 skipped: already exists  \n",
      "\u001b[2A\u001b[JCopying blob 63cd35141f3a skipped: already exists  \n",
      "Copying blob 7d676dc8a994 skipped: already exists  \n",
      "Copying blob 14726c8f7834 skipped: already exists  \n",
      "\u001b[3A\u001b[JCopying blob 63cd35141f3a skipped: already exists  \n",
      "Copying blob 7d676dc8a994 skipped: already exists  \n",
      "Copying blob 14726c8f7834 skipped: already exists  \n",
      "Copying blob 03bdd165d0d2 skipped: already exists  \n",
      "\u001b[4A\u001b[JCopying blob 63cd35141f3a skipped: already exists  \n",
      "Copying blob 7d676dc8a994 skipped: already exists  \n",
      "Copying blob 14726c8f7834 skipped: already exists  \n",
      "Copying blob 03bdd165d0d2 skipped: already exists  \n",
      "Copying blob 428bad6fa242 skipped: already exists  \n",
      "Copying config 84be3abda9 done  \n",
      "Writing manifest to image destination\n",
      "Storing signatures\n",
      "2023/08/26 11:51:42  info unpack layer: sha256:14726c8f78342865030f97a8d3492e2d1a68fbd22778f9a31dc6be4b4f12a9bc\n",
      "2023/08/26 11:51:43  info unpack layer: sha256:7d676dc8a994c0e4184cd6cfbc4b1540eea31846375a23864db3bb9effab24c0\n",
      "2023/08/26 11:51:43  info unpack layer: sha256:03bdd165d0d2a0b3b058a615aa395cebf5f94db698b2ee703a53a0bb6365804b\n",
      "2023/08/26 11:51:44  info unpack layer: sha256:428bad6fa2424960758fa690720e615f635324a1f04f82ab7974af4877c00872\n",
      "2023/08/26 11:51:44  info unpack layer: sha256:63cd35141f3a0351516d58c10780f4699e803265aa797e9a20389c3d743bf473\n",
      "\u001b[34mINFO:   \u001b[0m Copying . to /opt/app\n",
      "\u001b[34mINFO:   \u001b[0m Running post scriptlet\n",
      "+ python -m pip install -e /opt/app\n",
      "Obtaining file:///opt/app\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.24.2-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow->app==1.0) (65.5.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->app==1.0) (0.41.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, libclang, flatbuffers, wrapt, urllib3, tzdata, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, python-dateutil, pyasn1-modules, opt-einsum, h5py, google-pasta, astunparse, requests-oauthlib, pandas, google-auth, google-auth-oauthlib, tensorboard, tensorflow, app\n",
      "  Running setup.py develop for app\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-1.4.0 app-1.0 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.2.0 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 h5py-3.9.0 idna-3.4 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 pandas-2.0.3 protobuf-4.24.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 python-dateutil-2.8.2 pytz-2023.3 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 termcolor-2.3.0 typing-extensions-4.5.0 tzdata-2023.3 urllib3-1.26.16 werkzeug-2.3.7 wrapt-1.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[34mINFO:   \u001b[0m Creating SIF file...\n",
      "\u001b[34mINFO:   \u001b[0m Build complete: app.sif\n"
     ]
    }
   ],
   "source": [
    "! apptainer build -f app.sif app.def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our app inside the container, which has taken care of building all of our suprisingly complex dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Thom\n"
     ]
    }
   ],
   "source": [
    "! apptainer exec app.sif python /opt/app/app Thom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to submit a batch of greetings across the computing grid, we could deploy this local Apptainer image via Condor using the following submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe \u001b[38;5;241m=\u001b[39m vanilla\n",
      "\n",
      "executable \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m/\u001b[39musr\u001b[38;5;241m/\u001b[39mlocal\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m/\u001b[39mpython\n",
      "arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/app/app Thom\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "transfer_executable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "MY\u001b[38;5;241m.\u001b[39mSingularityImage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ENV(PWD)/app.sif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "requirements \u001b[38;5;241m=\u001b[39m (HAS_SINGULARITY\u001b[38;5;241m=\u001b[39m?\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "output \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m-\u001b[39m$(ProcId)\u001b[38;5;241m.\u001b[39mout\n",
      "error \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m-\u001b[39m$(ProcId)\u001b[38;5;241m.\u001b[39merr\n",
      "log \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m-\u001b[39m$(ProcId)\u001b[38;5;241m.\u001b[39mlog\n",
      "\n",
      "request_cpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "request_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\n",
      "queue \u001b[38;5;241m4\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "print_file(\"app.sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job(s)....\n",
      "4 job(s) submitted to cluster 410084.\n"
     ]
    }
   ],
   "source": [
    "! condor_submit app.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Jobs complete! Results:\n",
      "\n",
      "Job 0:  Hello Thom\n",
      "\n",
      "Job 1:  Hello Thom\n",
      "\n",
      "Job 2:  Hello Thom\n",
      "\n",
      "Job 3:  Hello Thom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "\n",
    "# hacky way to wait for all the jobs to complete\n",
    "while True:\n",
    "    for i in range(4):\n",
    "        if not os.path.exists(f\"app-{i}.out\"):\n",
    "            break\n",
    "        with open(f\"app-{i}.out\", \"r\") as f:\n",
    "            if not f.read():\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "    print(\"Waiting...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"Jobs complete! Results:\\n\")\n",
    "for i in range(4):\n",
    "    with open(f\"app-{i}.out\", \"r\") as f:\n",
    "        print(f\"Job {i}: \", f.read())\n",
    "    for suffix in [\"out\", \"err\", \"log\"]:\n",
    "        os.remove(f\"app-{i}.{suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're happy with our application, we can follow the steps [here](https://computing.docs.ligo.org/guide/dhtc/containers/#publishing) to make this container available at `/cvmfs/singularity.opensciencegrid.org` so that any other users who want to send greetings can just create submit files that point at our container and be good to go!\n",
    "\n",
    "So far so good. But what if I'm a user who thinks that the built-in greeting is too formal? Rather than saying \"Hello,\" I'd like to just say \"Hi.\" I would clone the repo, `git checkout -b less-formal-greeting`, and make the trivial code change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"app/__main__.py\", \"r\") as f:\n",
    "    script = f.read()\n",
    "script = script.replace(\"Hello\", \"Hi\")\n",
    "with open(\"app/__main__.py\", \"w\") as f:\n",
    "    f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-run our local container with the newly updated code (we'll keep using our local container, but in principle you would point to the one at `/cvmfs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Thom\n"
     ]
    }
   ],
   "source": [
    "! apptainer exec app.sif python /opt/app/app Thom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, it's still being too formal! That's because we edited our local copy of the Python code, but the code that lives inside the container is still the code we copied into it at build time! We could rebuild the container again, copying in the new code and then re-installing the package, but the build itself took around a minute. That seems like a lot of time to waste for such a simple change.\n",
    "\n",
    "Luckily, there's an even simpler, and cleaner, fix for this. We installed our application _editably_ (that's the `-e` in the `pip install` command in the Apptainer def file), so we can just mount our local copy of the code into the container at the appropriate location and our changes will automatically be reflected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Thom\n"
     ]
    }
   ],
   "source": [
    "! apptainer exec -B .:/opt/app app.sif python /opt/app/app Thom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we want to see how these changes look when we distribute them with Condor, we have no way of mounting our local directory to the appropriate place inside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job(s)....\n",
      "4 job(s) submitted to cluster 410085.\n"
     ]
    }
   ],
   "source": [
    "! condor_submit app.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Waiting...\n",
      "Jobs complete! Results:\n",
      "\n",
      "Job 0:  Hello Thom\n",
      "\n",
      "Job 1:  Hello Thom\n",
      "\n",
      "Job 2:  Hello Thom\n",
      "\n",
      "Job 3:  Hello Thom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    for i in range(4):\n",
    "        if not os.path.exists(f\"app-{i}.out\"):\n",
    "            break\n",
    "        with open(f\"app-{i}.out\", \"r\") as f:\n",
    "            if not f.read():\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "    print(\"Waiting...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"Jobs complete! Results:\\n\")\n",
    "for i in range(4):\n",
    "    with open(f\"app-{i}.out\", \"r\") as f:\n",
    "        print(f\"Job {i}: \", f.read())\n",
    "    for suffix in [\"out\", \"err\", \"log\"]:\n",
    "        os.remove(f\"app-{i}.{suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted [here](https://htcondor.readthedocs.io/en/latest/admin-manual/singularity-support.html), there is a Condor submit argument called `container_target_dir` that would support such functionality, but it's not currently enabled on LDG. That means the options for enabling development of distributed applications inside containers are (roughly in order of desirability):\n",
    "\n",
    "1. Removing the `MY.SingularityImage` argument from our submit file, setting the `executable` to `apptainer`, then specifying the desired bind flags ourselves as `arguments`, probably through some wrapper. This will probably be the easiest path in the short term, but I'd prefer to use Condor's built-in Apptainer support since I'm sure there are lots of little things that can go wrong that they've thought of that I never will.\n",
    "2. Maintaining base images in `/cvmfs/singularity.opensciencegrid.org` that have installed the necessary dependencies and take care of other more intensive environment set up, then have local def files that boostrap from these images do the comparatively lightweight install of the local package. Besides being annoying to have to run an extra `apptainer build` command every time you make a minor change to the code, this has the danger of becoming really messy as folks start needing to make changes to the base containers. In this scenario, they edit the def files of the base images then rebuild them locally, then have to edit the `Bootstrap` header of the package def file to point to these newly built images. Then they need to be sure to change the header back before pushing their code, and meanwhile the built final image has lost a lot of the \"self-contained\" nature that conatiners are designed for in the first place.\n",
    "3. Just rebuilding the entire container for every minor change. This keeps the def files clean, but for most practical environments would be so unwieldy that to me it doesn't represent a realistic option.\n",
    "4. Use build args to establish where in the container the package code should get copied so that folks can point to their home directory (which automatically gets bound in at run time). This would make everyone's build of the container, and even builds from different locations for the same user, completely different, and it would be impossible to build a single container that all users could use, which is sort of the whole idea to begin with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aframe-notebook",
   "language": "python",
   "name": "aframe-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
